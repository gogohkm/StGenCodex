"use strict";
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation and GitHub. All rights reserved.
 *--------------------------------------------------------------------------------------------*/
Object.defineProperty(exports, "__esModule", { value: true });
exports.sendChatParticipantRequest = sendChatParticipantRequest;
const prompt_tsx_1 = require("@vscode/prompt-tsx");
const vscode = require("vscode");
const toolsPrompt_1 = require("./toolsPrompt");
/**
 * Send a chat request, do the tool calling loop if needed, and return a stream and a ChatResult. Caller handles the response stream.
 */
function sendChatParticipantRequest(request, context, options, token) {
    let promise;
    const readable = new ReadableStream({
        start(controller) {
            promise = _sendChatParticipantRequest(controller, request, context, options, token).finally(() => controller.close());
            return promise;
        },
    });
    return {
        result: promise,
        stream: readable,
    };
}
async function _sendChatParticipantRequest(stream, request, context, options, token) {
    let model = options.model ?? request.model;
    if (options.tools?.length && model.vendor === 'copilot' && model.family.startsWith('o1')) {
        // The o1 models do not currently support tools
        const models = await vscode.lm.selectChatModels({
            vendor: 'copilot',
            family: 'gpt-4o',
        });
        model = models[0];
    }
    // Use all tools, or tools with the tags that are relevant.
    const tools = options.tools;
    const requestOptions = {
        justification: options.requestJustification,
    };
    // Render the initial prompt
    const result = await renderToolUserPrompt(model, {
        context,
        request,
        toolCallRounds: [],
        toolCallResults: {},
        libUserPrompt: options.prompt,
        tools,
    }, options.responseStreamOptions?.stream, options.extensionMode === vscode.ExtensionMode.Development);
    let messages = (0, prompt_tsx_1.toVsCodeChatMessages)(result.messages);
    result.references.forEach(ref => {
        if (ref.anchor instanceof vscode.Uri || ref.anchor instanceof vscode.Location) {
            if (options.responseStreamOptions?.references) {
                options.responseStreamOptions?.stream.reference(ref.anchor);
            }
        }
    });
    const toolReferences = [...request.toolReferences];
    const accumulatedToolResults = {};
    const toolCallRounds = [];
    const runWithTools = async () => {
        // If a toolReference is present, force the model to call that tool
        const requestedTool = toolReferences.shift();
        if (requestedTool) {
            requestOptions.toolMode = vscode.LanguageModelChatToolMode.Required;
            requestOptions.tools = vscode.lm.tools.filter(tool => tool.name === requestedTool.name);
        }
        else {
            requestOptions.toolMode = undefined;
            requestOptions.tools = tools ? [...tools] : undefined;
        }
        // Send the request to the LanguageModelChat
        const response = await model.sendRequest(messages, requestOptions, token);
        // Stream text output and collect tool calls from the response
        const toolCalls = [];
        let responseStr = '';
        for await (const part of response.stream) {
            if (part instanceof vscode.LanguageModelTextPart) {
                stream.enqueue(part);
                responseStr += part.value;
                if (options.responseStreamOptions?.responseText) {
                    options.responseStreamOptions.stream.markdown(part.value);
                }
            }
            else if (part instanceof vscode.LanguageModelToolCallPart) {
                toolCalls.push(part);
            }
        }
        if (toolCalls.length) {
            // If the model called any tools, then we do another round- render the prompt with those tool calls (rendering the PromptElements will invoke the tools)
            // and include the tool results in the prompt for the next request.
            toolCallRounds.push({
                response: responseStr,
                toolCalls,
            });
            const result = await renderToolUserPrompt(model, {
                context,
                request,
                toolCallRounds,
                toolCallResults: accumulatedToolResults,
                libUserPrompt: options.prompt,
                tools,
            }, options.responseStreamOptions?.stream, options.extensionMode === vscode.ExtensionMode.Development);
            messages = (0, prompt_tsx_1.toVsCodeChatMessages)(result.messages);
            const toolResultMetadata = result.metadata.getAll(toolsPrompt_1.ToolResultMetadata);
            if (toolResultMetadata?.length) {
                // Cache tool results for later, so they can be incorporated into later prompts without calling the tool again
                toolResultMetadata.forEach(meta => (accumulatedToolResults[meta.toolCallId] = meta.result));
            }
            // This loops until the model doesn't want to call any more tools, then the request is done.
            return runWithTools();
        }
    };
    await runWithTools();
    return {
        metadata: {
            // Return tool call metadata so it can be used in prompt history on the next request
            toolCallsMetadata: {
                toolCallResults: accumulatedToolResults,
                toolCallRounds,
            },
        },
    };
}
async function renderToolUserPrompt(chat, props, stream, serveTrace) {
    const renderer = new prompt_tsx_1.PromptRenderer({ modelMaxPromptTokens: chat.maxInputTokens }, toolsPrompt_1.ToolUserPrompt, props, {
        tokenLength: async (text, _token) => {
            return chat.countTokens(text);
        },
        countMessageTokens: async (message) => {
            return chat.countTokens(message.content);
        },
    });
    const tracer = new prompt_tsx_1.HTMLTracer();
    renderer.tracer = tracer;
    const result = await renderer.render();
    if (serveTrace) {
        const server = await tracer.serveHTML();
        if (stream) {
            const md = new vscode.MarkdownString('$(info) [View prompt trace](' + server.address + ')');
            md.supportThemeIcons = true;
            stream.markdown(md);
        }
        else {
            console.log('Prompt trace address:', server.address);
        }
    }
    return result;
}
