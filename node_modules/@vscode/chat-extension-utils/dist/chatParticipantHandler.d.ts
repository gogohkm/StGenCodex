import { PromptElement } from '@vscode/prompt-tsx';
import * as vscode from 'vscode';
import { AdHocChatTool, PromptElementAndProps } from './toolsPrompt';
export interface ChatHandlerOptions<T extends PromptElement = PromptElement> {
    /**
     * Instructions/"personality" for the chat participant prompt. This is what makes this chat participant different from others.
     */
    prompt?: string | PromptElementAndProps<T>;
    /**
     * If not specified, the user-selected model on ChatRequest will be used.
     */
    model?: vscode.LanguageModelChat;
    /**
     * An optional list of tools to use for this request.
     */
    tools?: ReadonlyArray<vscode.LanguageModelChatTool | AdHocChatTool<object>>;
    /**
     * See {@link vscode.LanguageModelChatRequestOptions.justification}
     */
    requestJustification?: string;
    /**
     * sendChatParticipantRequest returns a response stream, and the caller can handle streaming the response, or this option can
     * be used to enable sendChatParticipantRequest to stream the response back to VS Code. In that case, the chat participant
     * code doesn't have to handle the stream to return a chat response to VS Code.
     */
    responseStreamOptions?: {
        /**
         * The chat participant's stream, passed to the {@link vscode.ChatRequestHandler}.
         */
        stream: vscode.ChatResponseStream;
        /**
         * If true, sendChatParticipantRequest will automatically send references to the response stream.
         * @see {@link vscode.ChatResponseReferencePart}.
         */
        references?: boolean;
        /**
         * If true, sendChatParticipantRequest will automatically send the text response to the response stream.
         * @see {@link vscode.ChatResponseMarkdownPart}.
         */
        responseText?: boolean;
    };
    /**
     * Provide this from {@link vscode.ExtensionContext} so that sendChatParticipantRequest can check whether your extension is
     * running in debug mode. If it is, then a trace of the rendered prompt will be served. This trace is useful for seeing the
     * final prompt and understanding how it was rendered.
     *
     * If {@link ChatHandlerOptions.responseStreamOptions.stream} is also provided, a link to the trace will be added to the
     * response. Otherwise, the link to the trace will be logged to the console.
     */
    extensionMode?: vscode.ExtensionMode;
}
/**
 * The result of sendChatParticipantRequest.
 */
export interface ChatHandlerResult {
    /**
     * This is the ChatResult, which must be returned from the chat participant handler because it may contain error message
     * details or tool calling metadata.
     */
    result: Promise<vscode.ChatResult>;
    /**
     * This is a stream of the result text, and the results of tool calls. If
     * {@link ChatHandlerOptions.responseStreamOptions.responseText} is used, then this stream can be ignored. If not, then you
     * likely want to read this stream and send the text response back to VS Code via {@link vscode.ChatResponseStream}.
     */
    stream: AsyncIterable<vscode.LanguageModelTextPart | vscode.LanguageModelToolResult>;
}
/**
 * Send a chat request, do the tool calling loop if needed, and return a stream and a ChatResult. Caller handles the response stream.
 */
export declare function sendChatParticipantRequest(request: vscode.ChatRequest, context: vscode.ChatContext, options: ChatHandlerOptions, token: vscode.CancellationToken): ChatHandlerResult;
