"use strict";
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation and GitHub. All rights reserved.
 *--------------------------------------------------------------------------------------------*/
var __createBinding = (this && this.__createBinding) || (Object.create ? (function(o, m, k, k2) {
    if (k2 === undefined) k2 = k;
    var desc = Object.getOwnPropertyDescriptor(m, k);
    if (!desc || ("get" in desc ? !m.__esModule : desc.writable || desc.configurable)) {
      desc = { enumerable: true, get: function() { return m[k]; } };
    }
    Object.defineProperty(o, k2, desc);
}) : (function(o, m, k, k2) {
    if (k2 === undefined) k2 = k;
    o[k2] = m[k];
}));
var __exportStar = (this && this.__exportStar) || function(m, exports) {
    for (var p in m) if (p !== "default" && !Object.prototype.hasOwnProperty.call(exports, p)) __createBinding(exports, m, p);
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.contentType = exports.PromptRenderer = exports.MetadataMap = exports.PromptElement = exports.useKeepWith = exports.ToolResult = exports.UserMessage = exports.ToolMessage = exports.TextChunk = exports.SystemMessage = exports.PrioritizedList = exports.LegacyPrioritization = exports.FunctionMessage = exports.Chunk = exports.AssistantMessage = exports.ChatRole = exports.JSONTree = void 0;
exports.renderPrompt = renderPrompt;
exports.renderElementJSON = renderElementJSON;
exports.toVsCodeChatMessages = toVsCodeChatMessages;
const openai_1 = require("./openai");
const promptRenderer_1 = require("./promptRenderer");
const tokenizer_1 = require("./tokenizer/tokenizer");
__exportStar(require("./htmlTracer"), exports);
exports.JSONTree = require("./jsonTypes");
var openai_2 = require("./openai");
Object.defineProperty(exports, "ChatRole", { enumerable: true, get: function () { return openai_2.ChatRole; } });
__exportStar(require("./results"), exports);
__exportStar(require("./tracer"), exports);
__exportStar(require("./tsx-globals"), exports);
__exportStar(require("./types"), exports);
var promptElements_1 = require("./promptElements");
Object.defineProperty(exports, "AssistantMessage", { enumerable: true, get: function () { return promptElements_1.AssistantMessage; } });
Object.defineProperty(exports, "Chunk", { enumerable: true, get: function () { return promptElements_1.Chunk; } });
Object.defineProperty(exports, "FunctionMessage", { enumerable: true, get: function () { return promptElements_1.FunctionMessage; } });
Object.defineProperty(exports, "LegacyPrioritization", { enumerable: true, get: function () { return promptElements_1.LegacyPrioritization; } });
Object.defineProperty(exports, "PrioritizedList", { enumerable: true, get: function () { return promptElements_1.PrioritizedList; } });
Object.defineProperty(exports, "SystemMessage", { enumerable: true, get: function () { return promptElements_1.SystemMessage; } });
Object.defineProperty(exports, "TextChunk", { enumerable: true, get: function () { return promptElements_1.TextChunk; } });
Object.defineProperty(exports, "ToolMessage", { enumerable: true, get: function () { return promptElements_1.ToolMessage; } });
Object.defineProperty(exports, "UserMessage", { enumerable: true, get: function () { return promptElements_1.UserMessage; } });
Object.defineProperty(exports, "ToolResult", { enumerable: true, get: function () { return promptElements_1.ToolResult; } });
Object.defineProperty(exports, "useKeepWith", { enumerable: true, get: function () { return promptElements_1.useKeepWith; } });
var promptElement_1 = require("./promptElement");
Object.defineProperty(exports, "PromptElement", { enumerable: true, get: function () { return promptElement_1.PromptElement; } });
var promptRenderer_2 = require("./promptRenderer");
Object.defineProperty(exports, "MetadataMap", { enumerable: true, get: function () { return promptRenderer_2.MetadataMap; } });
Object.defineProperty(exports, "PromptRenderer", { enumerable: true, get: function () { return promptRenderer_2.PromptRenderer; } });
async function renderPrompt(ctor, props, endpoint, tokenizerMetadata, progress, token, mode = 'vscode') {
    let tokenizer = 'countTokens' in tokenizerMetadata
        ? new tokenizer_1.AnyTokenizer((text, token) => tokenizerMetadata.countTokens(text, token), mode)
        : tokenizerMetadata;
    const renderer = new promptRenderer_1.PromptRenderer(endpoint, ctor, props, tokenizer);
    const renderResult = await renderer.render(progress, token);
    const { tokenCount, references, metadata } = renderResult;
    let messages = renderResult.messages;
    const usedContext = renderer.getUsedContext();
    if (mode === 'vscode') {
        messages = toVsCodeChatMessages(messages);
    }
    return { messages, tokenCount, metadatas: metadata, metadata, usedContext, references };
}
/**
 * Content type of the return value from {@link renderElementJSON}.
 * When responding to a tool invocation, the tool should set this as the
 * content type in the returned data:
 *
 * ```ts
 * import { contentType } from '@vscode/prompt-tsx';
 *
 * async function doToolInvocation(): vscode.LanguageModelToolResult {
 *   return {
 *     [contentType]: await renderElementJSON(...),
 *     toString: () => '...',
 *   };
 * }
 * ```
 */
exports.contentType = 'application/vnd.codechat.prompt+json.1';
/**
 * Renders a prompt element to a serializable state. This type be returned in
 * tools results and reused in subsequent render calls via the `<Tool />`
 * element.
 *
 * In this mode, message chunks are not pruned from the tree; budget
 * information is used only to hint to the elements how many tokens they should
 * consume when rendered.
 *
 * @template P - The type of the prompt element props.
 * @param ctor - The constructor of the prompt element.
 * @param props - The props for the prompt element.
 * @param budgetInformation - Information about the token budget.
 * `vscode.LanguageModelToolInvocationOptions` is assignable to this object.
 * @param token - The cancellation token for cancelling the operation.
 * @returns A promise that resolves to an object containing the serialized data.
 */
function renderElementJSON(ctor, props, budgetInformation, token) {
    const renderer = new promptRenderer_1.PromptRenderer({ modelMaxPromptTokens: budgetInformation?.tokenBudget ?? Number.MAX_SAFE_INTEGER }, ctor, props, 
    // note: if tokenBudget is given, countTokens is also give and vise-versa.
    // `1` is used only as a dummy fallback to avoid errors if no/unlimited budget is provided.
    {
        countMessageTokens(message) {
            throw new Error('Tools may only return text, not messages.'); // for now...
        },
        tokenLength(text, token) {
            return Promise.resolve(budgetInformation?.countTokens(text, token) ?? Promise.resolve(1));
        },
    });
    return renderer.renderElementJSON(token);
}
/**
 * Converts an array of {@link ChatMessage} objects to an array of corresponding {@link LanguageModelChatMessage VS Code chat messages}.
 * @param messages - The array of {@link ChatMessage} objects to convert.
 * @returns An array of {@link LanguageModelChatMessage VS Code chat messages}.
 */
function toVsCodeChatMessages(messages) {
    const vscode = require('vscode');
    return messages.map(m => {
        switch (m.role) {
            case openai_1.ChatRole.Assistant:
                const message = vscode.LanguageModelChatMessage.Assistant(m.content, m.name);
                if (m.tool_calls) {
                    message.content = [
                        new vscode.LanguageModelTextPart(m.content),
                        ...m.tool_calls.map(tc => {
                            // prompt-tsx got args passed as a string, here we assume they are JSON because the vscode-type wants an object
                            let parsedArgs;
                            try {
                                parsedArgs = JSON.parse(tc.function.arguments);
                            }
                            catch (err) {
                                throw new Error('Invalid JSON in tool call arguments for tool call: ' + tc.id);
                            }
                            return new vscode.LanguageModelToolCallPart(tc.id, tc.function.name, parsedArgs);
                        }),
                    ];
                }
                return message;
            case openai_1.ChatRole.User:
                return vscode.LanguageModelChatMessage.User(m.content, m.name);
            case openai_1.ChatRole.Function: {
                const message = vscode.LanguageModelChatMessage.User('');
                message.content = [
                    new vscode.LanguageModelToolResultPart(m.name, [
                        new vscode.LanguageModelTextPart(m.content),
                    ]),
                ];
                return message;
            }
            case openai_1.ChatRole.Tool: {
                const message = vscode.LanguageModelChatMessage.User('');
                message.content = [
                    new vscode.LanguageModelToolResultPart(m.tool_call_id, [
                        new vscode.LanguageModelTextPart(m.content),
                    ]),
                ];
                return message;
            }
            default:
                throw new Error(`Converting chat message with role ${m.role} to VS Code chat message is not supported.`);
        }
    });
}
